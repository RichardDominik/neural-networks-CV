{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PNSPV-hw1-final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMmaPh6dMK+2IBrIE1W34oU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RichardDominik/neural-networks-CV/blob/master/PNSPV_hw1_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsWhx6tihCHy"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from torch.nn import Sequential, Linear, ReLU, Softmax, Conv2d, Dropout2d, Dropout, MaxPool2d, BatchNorm2d, BatchNorm1d, Flatten, CrossEntropyLoss, Sigmoid, Tanh, ELU, LeakyReLU, PReLU\n",
        "from torch.optim import Adam, SGD, RMSprop, AdamW\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx_OA1GQhMmU"
      },
      "source": [
        "Načítanie datasetu Cifar 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2C0oK3UhKNn"
      },
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "transform = transforms.ToTensor()\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainset, valset = torch.utils.data.random_split(dataset, [45000, 5000], generator=generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLu5qCkxhNm1"
      },
      "source": [
        "Zobrazenie obrázkov z datasetu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SukyDlV_hPcw"
      },
      "source": [
        "previewloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True)\n",
        "\n",
        "for i, sample in enumerate(previewloader):\n",
        "  if i > 1:\n",
        "    break\n",
        "  x, y = sample\n",
        "  img = np.moveaxis(x.numpy()[0], 0, -1)\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "  print(classes[y.item()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slEMqEvAhSYg"
      },
      "source": [
        "# Základný model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25H2adirhVXq"
      },
      "source": [
        "from torch.nn import Sequential, Linear, ReLU, Softmax, Conv2d, MaxPool2d, Flatten, CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "KERNEL_SIZE_CONV = 3\n",
        "STRIDE = 2\n",
        "PADDING = 1\n",
        "MAX_POOL_KERNEL = 2\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "NUMBER_OF_EPOCHS = 10\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def build_model():\n",
        "  model = Sequential(Conv2d(3, 16, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), ReLU(),\n",
        "                    Conv2d(16, 32, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), ReLU(),\n",
        "                    Conv2d(32, 64, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), ReLU(),\n",
        "                    Flatten(),\n",
        "                    Linear(1024, 512), ReLU(),\n",
        "                    Linear(512, 256), ReLU(),\n",
        "                    Linear(256, 128), ReLU(),\n",
        "                    Linear(128, 10))\n",
        "  \n",
        "  model_inference = Sequential(model, Softmax())\n",
        "  return model, model_inference\n",
        "\n",
        "\n",
        "model, model_inference = build_model()\n",
        "model, model_inference = model.to(device), model_inference.to(device)\n",
        "\n",
        "dataloader_train = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataloader_val = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "ce_loss = CrossEntropyLoss().to(device)\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "def one_epoch(model, loss, optimizer, dataloader_train, dataloader_val, device, verbose=True):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for i, batch in enumerate(dataloader_train):  \n",
        "    x, y = batch[0].to(device), batch[1].to(device) \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(x)\n",
        "\n",
        "    loss = ce_loss(out, y)\n",
        "    loss.backward()\n",
        "    train_losses.append(loss.item())\n",
        "    optimizer.step()\n",
        "    if i % 100 == 0 and verbose:\n",
        "      print(\"Training loss at step {}: {}\".format(i, loss.item()))\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, batch in enumerate(dataloader_val):  \n",
        "      x, y = batch[0].to(device), batch[1].to(device)  \n",
        "\n",
        "      out = model(x)\n",
        "      loss = ce_loss(out, y)\n",
        "      acc = torch.sum(torch.argmax(out, dim=-1) == y)\n",
        "      correct += acc.item()\n",
        "      total += len(batch[1])\n",
        "      val_losses.append(loss.item())\n",
        "\n",
        "  val_acc = correct / total\n",
        "\n",
        "  return np.mean(train_losses), np.mean(val_losses), val_acc\n",
        "\n",
        "for e in range(NUMBER_OF_EPOCHS):\n",
        "  train_loss, val_loss, val_acc = one_epoch(model, ce_loss, optimizer, dataloader_train, dataloader_val, device)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "\n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWrhpVzNheX3"
      },
      "source": [
        "# Aktivácie\n",
        "\n",
        "Funkcie pre budovanie modelov. Oskúšané aktivačné funkcie: Sigmoid, tanh, ELU, LeakyReLU a PReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we1Ah6Q0hfPx"
      },
      "source": [
        "from torch.nn import Sigmoid, Tanh, ELU, LeakyReLU, PReLU\n",
        "\n",
        "# Sigmoid\n",
        "def build_model_using_sigmoid_activation_function():\n",
        "  model = Sequential(Conv2d(3, 16, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), Sigmoid(),\n",
        "                  Conv2d(16, 32, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), Sigmoid(),\n",
        "                  Conv2d(32, 64, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), Sigmoid(),\n",
        "                  Flatten(),\n",
        "                  Linear(1024, 512), Sigmoid(),\n",
        "                  Linear(512, 256), Sigmoid(),\n",
        "                  Linear(256, 128), Sigmoid(),\n",
        "                  Linear(128, 10))\n",
        "\n",
        "  model_inference = Sequential(model, Softmax())\n",
        "  return model, model_inference\n",
        "\n",
        "# Tanh\n",
        "def build_model_using_tanh_activation_function():\n",
        "  model = Sequential(Conv2d(3, 16, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), Tanh(),\n",
        "                    Conv2d(16, 32, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), Tanh(),\n",
        "                    Conv2d(32, 64, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), Tanh(),\n",
        "                    Flatten(),\n",
        "                    Linear(1024, 512), Tanh(),\n",
        "                    Linear(512, 256), Tanh(),\n",
        "                    Linear(256, 128), Tanh(),\n",
        "                    Linear(128, 10))\n",
        "\n",
        "  model_inference = Sequential(model, Softmax())\n",
        "  return model, model_inference\n",
        "\n",
        "# ELU\n",
        "def build_model_using_elu_activation_function():\n",
        "  model = Sequential(Conv2d(3, 16, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), ELU(),\n",
        "                    Conv2d(16, 32, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), ELU(),\n",
        "                    Conv2d(32, 64, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), ELU(),\n",
        "                    Flatten(),\n",
        "                    Linear(1024, 512), ELU(),\n",
        "                    Linear(512, 256), ELU(),\n",
        "                    Linear(256, 128), ELU(),\n",
        "                    Linear(128, 10))\n",
        "\n",
        "  model_inference = Sequential(model, Softmax())\n",
        "  return model, model_inference\n",
        "\n",
        "# LeakyReLu\n",
        "ALPHA = 0.1\n",
        "\n",
        "def build_model_using_leaky_relu_activation_function():\n",
        "  model = Sequential(Conv2d(3, 16, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), LeakyReLU(ALPHA),\n",
        "                    Conv2d(16, 32, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), LeakyReLU(ALPHA),\n",
        "                    Conv2d(32, 64, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), LeakyReLU(ALPHA),\n",
        "                    Flatten(),\n",
        "                    Linear(1024, 512), LeakyReLU(ALPHA),\n",
        "                    Linear(512, 256), LeakyReLU(ALPHA),\n",
        "                    Linear(256, 128), LeakyReLU(ALPHA),\n",
        "                    Linear(128, 10))\n",
        "\n",
        "  model_inference = Sequential(model, Softmax())\n",
        "  return model, model_inference\n",
        "\n",
        "# PReLU\n",
        "def build_model_using_prelu_activation_function():\n",
        "  model = Sequential(Conv2d(3, 16, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), PReLU(),\n",
        "                    Conv2d(16, 32, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), PReLU(),\n",
        "                    Conv2d(32, 64, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL, STRIDE), PReLU(),\n",
        "                    Flatten(),\n",
        "                    Linear(1024, 512), PReLU(),\n",
        "                    Linear(512, 256), PReLU(),\n",
        "                    Linear(256, 128), PReLU(),\n",
        "                    Linear(128, 10))\n",
        "\n",
        "  model_inference = Sequential(model, Softmax())\n",
        "  return model, model_inference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54CYUTzahk34"
      },
      "source": [
        "Inicialializácia modelov podľa aktivačných funkcií"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQm7a_OfhjVr"
      },
      "source": [
        "# Sigmoid model\n",
        "sigmoid_model, sigmoid_model_inference = build_model_using_sigmoid_activation_function()\n",
        "sigmoid_model, sigmoid_model_inference = sigmoid_model.to(device), sigmoid_model_inference.to(device)\n",
        "\n",
        "# Tanh model\n",
        "tanh_model, tanh_model_inference = build_model_using_tanh_activation_function()\n",
        "tanh_model, tanh_model_inference = tanh_model.to(device), tanh_model_inference.to(device)\n",
        "\n",
        "# ELU model\n",
        "elu_model, elu_model_inference = build_model_using_elu_activation_function()\n",
        "elu_model, elu_model_inference = elu_model.to(device), elu_model_inference.to(device)\n",
        "\n",
        "# LeakyReLu model\n",
        "leaky_relu_model, leaky_relu_model_inference = build_model_using_leaky_relu_activation_function()\n",
        "leaky_relu_model, leaky_relu_model_inference = leaky_relu_model.to(device), leaky_relu_model_inference.to(device)\n",
        "\n",
        "# PReLU model\n",
        "prelu_model, prelu_model_inference = build_model_using_prelu_activation_function()\n",
        "prelu_model, prelu_model_inference = prelu_model.to(device), prelu_model_inference.to(device)\n",
        "\n",
        "# All models\n",
        "models = [('sigmoid', sigmoid_model), ('tanh', tanh_model), ('elu', elu_model), ('leaky relu', leaky_relu_model), ('prelu', prelu_model)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FppE2UGLhoR3"
      },
      "source": [
        "Trénovanie modelov"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0EJzIltho5-"
      },
      "source": [
        "for activation_function_name, model in models:\n",
        "  optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "  epoch_train_losses = []\n",
        "  epoch_val_losses = []\n",
        "  epoch_val_accs = []\n",
        "\n",
        "  print(activation_function_name) \n",
        "\n",
        "  for e in range(NUMBER_OF_EPOCHS):\n",
        "    train_loss, val_loss, val_acc = one_epoch(model, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "    print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "    print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "    epoch_train_losses.append(train_loss)\n",
        "    epoch_val_losses.append(val_loss)\n",
        "    epoch_val_accs.append(val_acc)\n",
        "\n",
        "  print('')\n",
        "\n",
        "  # pridat nazvy + legendy\n",
        "  plt.title('Loss: ' + activation_function_name)\n",
        "  plt.plot(epoch_train_losses, c='r')\n",
        "  plt.plot(epoch_val_losses, c='b')\n",
        "  plt.show()\n",
        "\n",
        "  plt.title('Acc: ' + activation_function_name)\n",
        "  plt.plot(epoch_val_accs, c='r')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF1_fLYshvua"
      },
      "source": [
        "# Optimalizácia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmsoNwwChqsO"
      },
      "source": [
        "from torch.optim import Adam, SGD, RMSprop, AdamW\n",
        "\n",
        "NUMBER_OF_OPT_EPOCHS = 15\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "baseline_model, baseline_model_inference = build_model()\n",
        "baseline_model, baseline_model_inference = baseline_model.to(device), baseline_model_inference.to(device)\n",
        "\n",
        "dataloader_train = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataloader_val = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "optimizers = [\n",
        "    ('Adam: lr= 1e-5', Adam(baseline_model.parameters(), lr=1e-5)),\n",
        "    ('SGD: lr= 1e-3', SGD(baseline_model.parameters(), lr=1e-3)),\n",
        "    ('RMSprop: lr= 1e-2, eps=1.1', RMSprop(baseline_model.parameters(), lr=1e-2, eps=1.1)),\n",
        "    ('AdamW: lr= 1e-3', AdamW(baseline_model.parameters(), lr=1e-3)),\n",
        "]\n",
        "\n",
        "for optDetails, optimizerTest in optimizers:\n",
        "  ce_loss = CrossEntropyLoss().to(device)\n",
        "  epoch_train_losses = []\n",
        "  epoch_val_losses = []\n",
        "  epoch_val_accs = []\n",
        "\n",
        "  print(optDetails) \n",
        "  print(optimizerTest)\n",
        "\n",
        "  for e in range(NUMBER_OF_OPT_EPOCHS):\n",
        "    train_loss, val_loss, val_acc = one_epoch(baseline_model, ce_loss, optimizerTest, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "    print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "    print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "    epoch_train_losses.append(train_loss)\n",
        "    epoch_val_losses.append(val_loss)\n",
        "    epoch_val_accs.append(val_acc)\n",
        "\n",
        "  print('')\n",
        "\n",
        "  plt.title('Loss: ' + optDetails)\n",
        "  plt.plot(epoch_train_losses, c='r')\n",
        "  plt.plot(epoch_val_losses, c='b')\n",
        "  plt.show()\n",
        "\n",
        "  plt.title('Acc: ' + optDetails)\n",
        "  plt.plot(epoch_val_accs, c='r')\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FHj3yqZh5pI"
      },
      "source": [
        "Batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC9yS-Lgh6JG"
      },
      "source": [
        "from torch.optim import Adam, SGD, RMSprop, AdamW\n",
        "batch_sizes = [8, 16, 64]\n",
        "NUMBER_OF_OPT_EPOCHS = 15\n",
        "\n",
        "for bs in batch_sizes:\n",
        "  baseline_model, baseline_model_inference = build_model()\n",
        "  baseline_model, baseline_model_inference = baseline_model.to(device), baseline_model_inference.to(device)\n",
        "  dataloader_train = DataLoader(trainset, batch_size=bs, shuffle=True)\n",
        "  dataloader_val = DataLoader(valset, batch_size=bs, shuffle=True)\n",
        "  optimizer = AdamW(baseline_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "  ce_loss = CrossEntropyLoss().to(device)\n",
        "  epoch_train_losses = []\n",
        "  epoch_val_losses = []\n",
        "  epoch_val_accs = []\n",
        "  print('Batch size: ' + str(bs))\n",
        "\n",
        "  for e in range(NUMBER_OF_OPT_EPOCHS):\n",
        "    train_loss, val_loss, val_acc = one_epoch(baseline_model, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "    print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "    print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "    epoch_train_losses.append(train_loss)\n",
        "    epoch_val_losses.append(val_loss)\n",
        "    epoch_val_accs.append(val_acc)\n",
        "\n",
        "  print('')\n",
        "\n",
        "  plt.title('Loss: ' + str(bs))\n",
        "  plt.plot(epoch_train_losses, c='r')\n",
        "  plt.plot(epoch_val_losses, c='b')\n",
        "  plt.show()\n",
        "\n",
        "  plt.title('Acc: ' + str(bs))\n",
        "  plt.plot(epoch_val_accs, c='r')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8TN77rih-wL"
      },
      "source": [
        "# Dropout a Augmentácia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax1ClTF8iUIz"
      },
      "source": [
        "def build_dropout_model_fully_connected(dropout_p=0.5):\n",
        "  modules = []\n",
        "  modules.append(Conv2d(3, 16, KERNEL_SIZE_CONV, 1, PADDING))\n",
        "  modules.append(MaxPool2d(MAX_POOL_KERNEL, STRIDE))\n",
        "  modules.append(ReLU())\n",
        "  modules.append(Conv2d(16, 32, KERNEL_SIZE_CONV, 1, PADDING))\n",
        "  modules.append(MaxPool2d(MAX_POOL_KERNEL, STRIDE))\n",
        "  modules.append(ReLU())\n",
        "  modules.append(Conv2d(32, 64, KERNEL_SIZE_CONV, 1, PADDING))\n",
        "  modules.append(MaxPool2d(MAX_POOL_KERNEL, STRIDE))\n",
        "  modules.append(ReLU())\n",
        "  modules.append(Flatten())\n",
        "\n",
        "  modules.append(Linear(1024, 512))\n",
        "  modules.append(ReLU())\n",
        "  if dropout_p:\n",
        "    modules.append(Dropout(dropout_p))\n",
        "\n",
        "  modules.append(Linear(512, 256))\n",
        "  modules.append(ReLU())\n",
        "  if dropout_p:\n",
        "    modules.append(Dropout(dropout_p))\n",
        "\n",
        "  modules.append(Linear(256, 128))\n",
        "  modules.append(ReLU())\n",
        "  if dropout_p:\n",
        "    modules.append(Dropout(dropout_p))\n",
        "\n",
        "  modules.append(Linear(128, 10))\n",
        "\n",
        "  model = Sequential(*modules)\n",
        "  model_inference = Sequential(model, Softmax())\n",
        "\n",
        "  return model, model_inference\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pzbh5XkiUtB"
      },
      "source": [
        "KERNEL_SIZE_CONV = 3\n",
        "STRIDE = 2\n",
        "PADDING = 1\n",
        "MAX_POOL_KERNEL = 2\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "trainset, valset = torch.utils.data.random_split(dataset, [45000, 5000], generator=generator)\n",
        "\n",
        "model_dropout_fully_connected, model_inference_dropout_fully_connected = build_dropout_model_fully_connected()\n",
        "model_dropout_fully_connected, model_inference_dropout_fully_connected = model_dropout_fully_connected.to(device), model_inference_dropout_fully_connected.to(device)\n",
        "\n",
        "dataloader_train = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataloader_val = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "ce_loss = CrossEntropyLoss().to(device)\n",
        "optimizer = Adam(model_dropout_fully_connected.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "for e in range(30):\n",
        "  train_loss, val_loss, val_acc = one_epoch(model_dropout_fully_connected, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "\n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBTFRseNiWFC"
      },
      "source": [
        "trainset, valset, _ = torch.utils.data.random_split(dataset, [1000, 5000, 44000], generator=generator)\n",
        "\n",
        "model_dropout_fully_connected, model_inference_dropout_fully_connected = build_dropout_model_fully_connected()\n",
        "model_dropout_fully_connected, model_inference_dropout_fully_connected = model_dropout_fully_connected.to(device), model_inference_dropout_fully_connected.to(device)\n",
        "\n",
        "dataloader_train = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataloader_val = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "ce_loss = CrossEntropyLoss().to(device)\n",
        "optimizer = Adam(model_dropout_fully_connected.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "for e in range(100):\n",
        "  train_loss, val_loss, val_acc = one_epoch(model_dropout_fully_connected, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "\n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1fPpVriiZSP"
      },
      "source": [
        "def build_dropout_model_conv_layers(dropout_p=0.5, droupout_conv = 0.2):\n",
        "  modules = []\n",
        "  modules.append(Conv2d(3, 16, KERNEL_SIZE_CONV, 1, PADDING))\n",
        "  modules.append(MaxPool2d(MAX_POOL_KERNEL, STRIDE))\n",
        "  modules.append(ReLU())\n",
        "  if droupout_conv:\n",
        "    modules.append(Dropout2d(droupout_conv))\n",
        "  \n",
        "  modules.append(Conv2d(16, 32, KERNEL_SIZE_CONV, 1, PADDING))\n",
        "  modules.append(MaxPool2d(MAX_POOL_KERNEL, STRIDE))\n",
        "  modules.append(ReLU())\n",
        "\n",
        "  if droupout_conv:\n",
        "    modules.append(Dropout2d(droupout_conv))\n",
        "\n",
        "  modules.append(Conv2d(32, 64, KERNEL_SIZE_CONV, 1, PADDING))\n",
        "  modules.append(MaxPool2d(MAX_POOL_KERNEL, STRIDE))\n",
        "  modules.append(ReLU())\n",
        "\n",
        "  if droupout_conv:\n",
        "    modules.append(Dropout2d(droupout_conv))\n",
        "\n",
        "  modules.append(Flatten())\n",
        "\n",
        "  modules.append(Linear(1024, 512))\n",
        "  modules.append(ReLU())\n",
        "  if dropout_p:\n",
        "    modules.append(Dropout(dropout_p))\n",
        "\n",
        "  modules.append(Linear(512, 256))\n",
        "  modules.append(ReLU())\n",
        "  if dropout_p:\n",
        "    modules.append(Dropout(dropout_p))\n",
        "\n",
        "  modules.append(Linear(256, 128))\n",
        "  modules.append(ReLU())\n",
        "  if dropout_p:\n",
        "    modules.append(Dropout(dropout_p))\n",
        "\n",
        "  modules.append(Linear(128, 10))\n",
        "\n",
        "  model = Sequential(*modules)\n",
        "  model_inference = Sequential(model, Softmax())\n",
        "\n",
        "  return model, model_inference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkNoTMm4iaaH"
      },
      "source": [
        "trainset, valset = torch.utils.data.random_split(dataset, [45000, 5000], generator=generator)\n",
        "\n",
        "model_dropout_conv_layers, model_inference_dropout_conv_layers = build_dropout_model_conv_layers()\n",
        "model_dropout_conv_layers, model_inference_dropout_conv_layers = model_dropout_conv_layers.to(device), model_inference_dropout_conv_layers.to(device)\n",
        "\n",
        "dataloader_train = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataloader_val = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "ce_loss = CrossEntropyLoss().to(device)\n",
        "optimizer = Adam(model_dropout_conv_layers.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "for e in range(40):\n",
        "  train_loss, val_loss, val_acc = one_epoch(model_dropout_conv_layers, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "\n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOl4pS7Eib_0"
      },
      "source": [
        "trainset, valset, _ = torch.utils.data.random_split(dataset, [1000, 5000, 44000], generator=generator)\n",
        "\n",
        "dataloader_train = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataloader_val = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "ce_loss = CrossEntropyLoss().to(device)\n",
        "optimizer = Adam(model_dropout_conv_layers.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "for e in range(100):\n",
        "  train_loss, val_loss, val_acc = one_epoch(model_dropout_conv_layers, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "\n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LElblNI2ieOe"
      },
      "source": [
        "def build_dropout_model_v3(dropout_p=0.5, droupout_conv = 0.2):\n",
        "  modules = []\n",
        "  modules.append(Conv2d(3, 16, KERNEL_SIZE_CONV, 1, PADDING))\n",
        "  if droupout_conv:\n",
        "    modules.append(Dropout2d(droupout_conv))\n",
        "\n",
        "  modules.append(MaxPool2d(MAX_POOL_KERNEL, STRIDE))\n",
        "  modules.append(ReLU())\n",
        "  \n",
        "  modules.append(Conv2d(16, 32, KERNEL_SIZE_CONV, 1, PADDING))\n",
        "\n",
        "  if droupout_conv:\n",
        "    modules.append(Dropout2d(droupout_conv))\n",
        "\n",
        "  modules.append(MaxPool2d(MAX_POOL_KERNEL, STRIDE))\n",
        "  modules.append(ReLU())\n",
        "\n",
        "  modules.append(Conv2d(32, 64, KERNEL_SIZE_CONV, 1, PADDING))\n",
        "\n",
        "  if droupout_conv:\n",
        "    modules.append(Dropout2d(droupout_conv))\n",
        "\n",
        "  modules.append(MaxPool2d(MAX_POOL_KERNEL, STRIDE))\n",
        "  modules.append(ReLU())\n",
        "\n",
        "  modules.append(Flatten())\n",
        "\n",
        "  modules.append(Linear(1024, 512))\n",
        "  modules.append(ReLU())\n",
        "\n",
        "  if dropout_p:\n",
        "    modules.append(Dropout(dropout_p))\n",
        " \n",
        "  modules.append(Linear(512, 256))\n",
        "  modules.append(ReLU())\n",
        "\n",
        "  if dropout_p:\n",
        "    modules.append(Dropout(dropout_p))\n",
        "\n",
        "  modules.append(Linear(256, 128))\n",
        "  modules.append(ReLU())\n",
        "  if dropout_p:\n",
        "    modules.append(Dropout(dropout_p))\n",
        "\n",
        "  modules.append(Linear(128, 10))\n",
        "\n",
        "  model = Sequential(*modules)\n",
        "  model_inference = Sequential(model, Softmax())\n",
        "\n",
        "  return model, model_inference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH_N9f_Fige8"
      },
      "source": [
        "trainset, valset = torch.utils.data.random_split(dataset, [45000, 5000], generator=generator)\n",
        "\n",
        "model_dropout_v3, model_inference_dropout_v3 = build_dropout_model_v3()\n",
        "model_dropout_v3, model_inference_dropout_v3 = model_dropout_v3.to(device), model_inference_dropout_v3.to(device)\n",
        "\n",
        "dataloader_train = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataloader_val = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "ce_loss = CrossEntropyLoss().to(device)\n",
        "optimizer = Adam(model_dropout_v3.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "for e in range(40):\n",
        "  train_loss, val_loss, val_acc = one_epoch(model_dropout_v3, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "\n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1jXanHsiiMj"
      },
      "source": [
        "trainset, valset, _ = torch.utils.data.random_split(dataset, [1000, 5000, 44000], generator=generator)\n",
        "\n",
        "model_dropout_v3, model_inference_dropout_v3 = build_dropout_model_v3()\n",
        "model_dropout_v3, model_inference_dropout_v3 = model_dropout_v3.to(device), model_inference_dropout_v3.to(device)\n",
        "\n",
        "dataloader_train = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataloader_val = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "ce_loss = CrossEntropyLoss().to(device)\n",
        "optimizer = Adam(model_dropout_v3.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "for e in range(100):\n",
        "  train_loss, val_loss, val_acc = one_epoch(model_dropout_v3, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "\n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1WB0twUikDP"
      },
      "source": [
        "# prva augmentacia\n",
        "aug_transforms_first = transforms.Compose([transforms.RandomRotation(degrees=(0, 180)),\n",
        "                                     transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                     transforms.RandomVerticalFlip(p=0.5),\n",
        "                                     transforms.RandomResizedCrop(size=(32, 32)),\n",
        "                                     transforms.ToTensor()])\n",
        "\n",
        "dataset_aug_first = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=aug_transforms_first)\n",
        "trainset_aug_first, _ = torch.utils.data.random_split(dataset_aug_first, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
        "dataloader_preview_aug_first = torch.utils.data.DataLoader(trainset_aug_first, batch_size=1, shuffle=False)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(trainset_aug_first, batch_size=batch_size, shuffle=True)\n",
        "dataloader_val = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device: \", device)\n",
        "\n",
        "ce_loss = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "baseline_model, baseline_model_inference = build_model()\n",
        "\n",
        "baseline_model.to(device)\n",
        "baseline_model_inference.to(device)\n",
        "\n",
        "optimizer = Adam(baseline_model.parameters(), lr=1e-3)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "for e in range(30):\n",
        "  train_loss, val_loss, val_acc = one_epoch(baseline_model, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  torch.save(model.state_dict(), \"{:03d}.pth\".format(e))\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "  \n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCoJiBNVilyz"
      },
      "source": [
        "# prva augmentacia iba 1000\n",
        "aug_transforms_first = transforms.Compose([transforms.RandomRotation(degrees=(0, 180)),\n",
        "                                     transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                     transforms.RandomVerticalFlip(p=0.5),\n",
        "                                     transforms.RandomResizedCrop(size=(32, 32)),\n",
        "                                     transforms.ToTensor()])\n",
        "\n",
        "dataset_aug_first = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=aug_transforms_first)\n",
        "trainset_aug_first, _, _ = torch.utils.data.random_split(dataset_aug_first, [1000, 5000, 44000], generator=torch.Generator().manual_seed(42))\n",
        "dataloader_preview_aug_first = torch.utils.data.DataLoader(trainset_aug_first, batch_size=1, shuffle=False)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(trainset_aug_first, batch_size=batch_size, shuffle=True)\n",
        "dataloader_val = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device: \", device)\n",
        "\n",
        "ce_loss = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "baseline_model, baseline_model_inference = build_model()\n",
        "\n",
        "baseline_model.to(device)\n",
        "baseline_model_inference.to(device)\n",
        "\n",
        "optimizer = Adam(baseline_model.parameters(), lr=1e-3)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "for e in range(100):\n",
        "  train_loss, val_loss, val_acc = one_epoch(baseline_model, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  torch.save(model.state_dict(), \"{:03d}.pth\".format(e))\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "  \n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy0fHS7yintN"
      },
      "source": [
        "# druha augmentacia\n",
        "aug_transforms_second = transforms.Compose([transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
        "                                     transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                     transforms.RandomResizedCrop(size=(32, 32)),\n",
        "                                     transforms.ToTensor()])\n",
        "\n",
        "dataset_aug_second = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=aug_transforms_second)\n",
        "trainset_aug_second, _ = torch.utils.data.random_split(dataset_aug_second, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
        "dataloader_preview_aug_second = torch.utils.data.DataLoader(trainset_aug_second, batch_size=1, shuffle=False)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(trainset_aug_second, batch_size=batch_size, shuffle=True)\n",
        "dataloader_val = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device: \", device)\n",
        "\n",
        "ce_loss = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "baseline_model, baseline_model_inference = build_model()\n",
        "\n",
        "baseline_model.to(device)\n",
        "baseline_model_inference.to(device)\n",
        "\n",
        "optimizer = Adam(baseline_model.parameters(), lr=1e-3)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "for e in range(30):\n",
        "  train_loss, val_loss, val_acc = one_epoch(baseline_model, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  torch.save(model.state_dict(), \"{:03d}.pth\".format(e))\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "  \n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ7wmlovipwd"
      },
      "source": [
        "# druha augmentacia iba 1000\n",
        "aug_transforms_second = transforms.Compose([transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
        "                                     transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                     transforms.RandomResizedCrop(size=(32, 32)),\n",
        "                                     transforms.ToTensor()])\n",
        "\n",
        "dataset_aug_second = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=aug_transforms_second)\n",
        "trainset_aug_second, _, _ = torch.utils.data.random_split(dataset_aug_second, [1000, 5000, 44000], generator=torch.Generator().manual_seed(42))\n",
        "dataloader_preview_aug_second = torch.utils.data.DataLoader(trainset_aug_second, batch_size=1, shuffle=False)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(trainset_aug_second, batch_size=batch_size, shuffle=True)\n",
        "dataloader_val = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device: \", device)\n",
        "\n",
        "ce_loss = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "baseline_model, baseline_model_inference = build_model()\n",
        "\n",
        "baseline_model.to(device)\n",
        "baseline_model_inference.to(device)\n",
        "\n",
        "optimizer = Adam(baseline_model.parameters(), lr=1e-3)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "for e in range(100):\n",
        "  train_loss, val_loss, val_acc = one_epoch(baseline_model, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  torch.save(model.state_dict(), \"{:03d}.pth\".format(e))\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "  \n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WId2AKGCirZq"
      },
      "source": [
        "# tretia augmentacia\n",
        "aug_transforms_third = transforms.Compose([transforms.ColorJitter(brightness=.5, hue=.3),\n",
        "                                     transforms.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75)),\n",
        "                                     transforms.ToTensor()])\n",
        "\n",
        "dataset_aug_third = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=aug_transforms_third)\n",
        "trainset_aug_third, _ = torch.utils.data.random_split(dataset_aug_third, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
        "dataloader_preview_aug_third = torch.utils.data.DataLoader(trainset_aug_third, batch_size=1, shuffle=False)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(trainset_aug_third, batch_size=batch_size, shuffle=True)\n",
        "dataloader_val = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device: \", device)\n",
        "\n",
        "ce_loss = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "baseline_model, baseline_model_inference = build_model()\n",
        "\n",
        "baseline_model.to(device)\n",
        "baseline_model_inference.to(device)\n",
        "\n",
        "optimizer = Adam(baseline_model.parameters(), lr=1e-3)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "for e in range(30):\n",
        "  train_loss, val_loss, val_acc = one_epoch(baseline_model, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  torch.save(model.state_dict(), \"{:03d}.pth\".format(e))\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "  \n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30bikKMEit7h"
      },
      "source": [
        "# tretia augmentacia iba 1000\n",
        "aug_transforms_third = transforms.Compose([transforms.ColorJitter(brightness=.5, hue=.3),\n",
        "                                     transforms.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75)),\n",
        "                                     transforms.ToTensor()])\n",
        "\n",
        "dataset_aug_third = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=aug_transforms_third)\n",
        "trainset_aug_third, _, _= torch.utils.data.random_split(dataset_aug_third, [1000, 5000, 44000], generator=torch.Generator().manual_seed(42))\n",
        "dataloader_preview_aug_third = torch.utils.data.DataLoader(trainset_aug_third, batch_size=1, shuffle=False)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(trainset_aug_third, batch_size=batch_size, shuffle=True)\n",
        "dataloader_val = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device: \", device)\n",
        "\n",
        "ce_loss = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "baseline_model, baseline_model_inference = build_model()\n",
        "\n",
        "baseline_model.to(device)\n",
        "baseline_model_inference.to(device)\n",
        "\n",
        "optimizer = Adam(baseline_model.parameters(), lr=1e-3)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "for e in range(100):\n",
        "  train_loss, val_loss, val_acc = one_epoch(baseline_model, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  torch.save(model.state_dict(), \"{:03d}.pth\".format(e))\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "  \n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC81MoT8izIx"
      },
      "source": [
        "# Hlboká sieť"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXGomRYZiyw9"
      },
      "source": [
        "KERNEL_SIZE_CONV = 3\n",
        "STRIDE = 2\n",
        "PADDING = 2\n",
        "MAX_POOL_KERNEL = 2\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def build_model():\n",
        "  model = Sequential(Conv2d(3, 32, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL), ReLU(),\n",
        "                      Conv2d(32, 64, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL), ReLU(),\n",
        "                      Conv2d(64, 128, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL), ReLU(),\n",
        "                      Conv2d(128, 256, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL), ReLU(),\n",
        "                      Conv2d(256, 512, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL), ReLU(),\n",
        "                      Conv2d(512, 10, 2),\n",
        "                      Flatten())\n",
        "  \n",
        "  model_inference = Sequential(model, Softmax())\n",
        "  return model, model_inference\n",
        "\n",
        "model, model_inference = build_model()\n",
        "model, model_inference = model.to(device), model_inference.to(device)\n",
        "\n",
        "dataloader_train = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataloader_val = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "ce_loss = CrossEntropyLoss().to(device)\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "def one_epoch(model, loss, optimizer, dataloader_train, dataloader_val, device, verbose=True):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for i, batch in enumerate(dataloader_train):  \n",
        "    x, y = batch[0].to(device), batch[1].to(device) \n",
        "    optimizer.zero_grad()\n",
        "    out = model(x)\n",
        "    loss = ce_loss(out, y)\n",
        "    loss.backward()\n",
        "    train_losses.append(loss.item())\n",
        "    optimizer.step()\n",
        "    if i % 100 == 0 and verbose:\n",
        "      print(\"Training loss at step {}: {}\".format(i, loss.item()))\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, batch in enumerate(dataloader_val):  \n",
        "      x, y = batch[0].to(device), batch[1].to(device)  \n",
        "\n",
        "      out = model(x)\n",
        "      loss = ce_loss(out, y)\n",
        "      acc = torch.sum(torch.argmax(out, dim=-1) == y)\n",
        "      correct += acc.item()\n",
        "      total += len(batch[1])\n",
        "      val_losses.append(loss.item())\n",
        "\n",
        "  val_acc = correct / total\n",
        "\n",
        "  return np.mean(train_losses), np.mean(val_losses), val_acc\n",
        "\n",
        "for e in range(20):\n",
        "  train_loss, val_loss, val_acc = one_epoch(model, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "\n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbO56R9TjAxi"
      },
      "source": [
        "KERNEL_SIZE_CONV = 3\n",
        "STRIDE = 2\n",
        "PADDING = 2\n",
        "MAX_POOL_KERNEL = 2\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def build_model():\n",
        "  model = Sequential(Conv2d(3, 32, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL), ReLU(),\n",
        "                      Conv2d(32, 64, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL), ReLU(),\n",
        "                      Conv2d(64, 128, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL), ReLU(),\n",
        "                      Conv2d(128, 256, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL), ReLU(),\n",
        "                      Conv2d(256, 512, KERNEL_SIZE_CONV, 1, PADDING), MaxPool2d(MAX_POOL_KERNEL), ReLU(),\n",
        "                      Flatten(),\n",
        "                      Linear(2048, 1024), ReLU(),\n",
        "                      Linear(1024, 512), ReLU(),\n",
        "                      Linear(512, 256), ReLU(),\n",
        "                      Linear(256, 128), ReLU(),\n",
        "                      Linear(128, 10))\n",
        "  \n",
        "  model_inference = Sequential(model, Softmax())\n",
        "  return model, model_inference\n",
        "\n",
        "\n",
        "model, model_inference = build_model()\n",
        "model, model_inference = model.to(device), model_inference.to(device)\n",
        "\n",
        "dataloader_train = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataloader_val = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "ce_loss = CrossEntropyLoss().to(device)\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "def one_epoch(model, loss, optimizer, dataloader_train, dataloader_val, device, verbose=True):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for i, batch in enumerate(dataloader_train):  \n",
        "    # i hovori o tom ktora davka\n",
        "   \n",
        "    # x je obrazok \n",
        "    # y je trieda do ktorej obr patri\n",
        "    x, y = batch[0].to(device), batch[1].to(device) \n",
        "    optimizer.zero_grad()\n",
        "    out = model(x)\n",
        "\n",
        "    loss = ce_loss(out, y)\n",
        "    loss.backward()\n",
        "    train_losses.append(loss.item())\n",
        "    optimizer.step()\n",
        "    if i % 100 == 0 and verbose:\n",
        "      print(\"Training loss at step {}: {}\".format(i, loss.item()))\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, batch in enumerate(dataloader_val):  \n",
        "      x, y = batch[0].to(device), batch[1].to(device)  \n",
        "\n",
        "      out = model(x)\n",
        "      loss = ce_loss(out, y)\n",
        "      acc = torch.sum(torch.argmax(out, dim=-1) == y)\n",
        "      correct += acc.item()\n",
        "      total += len(batch[1])\n",
        "      val_losses.append(loss.item())\n",
        "\n",
        "  val_acc = correct / total\n",
        "\n",
        "  return np.mean(train_losses), np.mean(val_losses), val_acc\n",
        "\n",
        "for e in range(20):\n",
        "  train_loss, val_loss, val_acc = one_epoch(model, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "\n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPLvpdyDjE0M"
      },
      "source": [
        "# batch norm\n",
        "\n",
        "KERNEL_SIZE_CONV = 3\n",
        "STRIDE = 2\n",
        "PADDING = 2\n",
        "MAX_POOL_KERNEL = 2\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def build_model():\n",
        "  model = Sequential(Conv2d(3, 32, KERNEL_SIZE_CONV, 1, PADDING), BatchNorm2d(32), MaxPool2d(MAX_POOL_KERNEL), ReLU(),\n",
        "                      Conv2d(32, 64, KERNEL_SIZE_CONV, 1, PADDING), BatchNorm2d(64), MaxPool2d(MAX_POOL_KERNEL), ReLU(),\n",
        "                      Conv2d(64, 128, KERNEL_SIZE_CONV, 1, PADDING), BatchNorm2d(128),MaxPool2d(MAX_POOL_KERNEL), ReLU(),\n",
        "                      Conv2d(128, 256, KERNEL_SIZE_CONV, 1, PADDING), BatchNorm2d(256), MaxPool2d(MAX_POOL_KERNEL), ReLU(),\n",
        "                      Conv2d(256, 512, KERNEL_SIZE_CONV, 1, PADDING), BatchNorm2d(512), MaxPool2d(MAX_POOL_KERNEL), ReLU(),\n",
        "                      Flatten(),\n",
        "                      Linear(2048, 1024), BatchNorm1d(1024), ReLU(),\n",
        "                      Linear(1024, 512), BatchNorm1d(512), ReLU(),\n",
        "                      Linear(512, 256), BatchNorm1d(256), ReLU(),\n",
        "                      Linear(256, 128), BatchNorm1d(128), ReLU(),\n",
        "                      Linear(128, 10))\n",
        "  \n",
        "  model_inference = Sequential(model, Softmax())\n",
        "  return model, model_inference\n",
        "\n",
        "\n",
        "model, model_inference = build_model()\n",
        "model, model_inference = model.to(device), model_inference.to(device)\n",
        "\n",
        "dataloader_train = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataloader_val = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "ce_loss = CrossEntropyLoss().to(device)\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "def one_epoch(model, loss, optimizer, dataloader_train, dataloader_val, device, verbose=True):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for i, batch in enumerate(dataloader_train):  \n",
        "    x, y = batch[0].to(device), batch[1].to(device) \n",
        "    optimizer.zero_grad()\n",
        "    out = model(x)\n",
        "\n",
        "    loss = ce_loss(out, y)\n",
        "    loss.backward()\n",
        "    train_losses.append(loss.item())\n",
        "    optimizer.step()\n",
        "    if i % 100 == 0 and verbose:\n",
        "      print(\"Training loss at step {}: {}\".format(i, loss.item()))\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, batch in enumerate(dataloader_val):  \n",
        "      x, y = batch[0].to(device), batch[1].to(device)  \n",
        "\n",
        "      out = model(x)\n",
        "      loss = ce_loss(out, y)\n",
        "      acc = torch.sum(torch.argmax(out, dim=-1) == y)\n",
        "      correct += acc.item()\n",
        "      total += len(batch[1])\n",
        "      val_losses.append(loss.item())\n",
        "\n",
        "  val_acc = correct / total\n",
        "\n",
        "  return np.mean(train_losses), np.mean(val_losses), val_acc\n",
        "\n",
        "for e in range(20):\n",
        "  train_loss, val_loss, val_acc = one_epoch(model, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "\n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7APSw1JXjXr-"
      },
      "source": [
        "# Najlepší model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FgnRgomjku5"
      },
      "source": [
        "def adjust_lr(optimizer, ep):\n",
        "    if ep < 10:\n",
        "        lr = 1e-4 * (ep + 1) / 2\n",
        "    elif ep < 40:\n",
        "        lr = 1e-3 \n",
        "    elif ep < 70:\n",
        "        lr = 1e-4 \n",
        "    elif ep < 100:\n",
        "        lr = 1e-5 \n",
        "    elif ep < 130:\n",
        "        lr = 1e-6\n",
        "    elif ep < 160:\n",
        "        lr = 1e-4 \n",
        "    else:\n",
        "        lr = 1e-5 \n",
        "    for p in optimizer.param_groups:\n",
        "        p['lr'] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTORVNTPjolQ"
      },
      "source": [
        "KERNEL_SIZE_CONV = 3\n",
        "PADDING = 2\n",
        "MAX_POOL_KERNEL = 2\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def build_model(dropout_p=0.5, droupout_conv = 0.2):\n",
        "  model = Sequential(Conv2d(3, 32, KERNEL_SIZE_CONV, 1, PADDING), BatchNorm2d(32), MaxPool2d(MAX_POOL_KERNEL), PReLU(), Dropout2d(droupout_conv),\n",
        "                    Conv2d(32, 64, KERNEL_SIZE_CONV, 1, PADDING), BatchNorm2d(64), MaxPool2d(MAX_POOL_KERNEL), PReLU(), Dropout2d(droupout_conv),\n",
        "                    Conv2d(64, 128, KERNEL_SIZE_CONV, 1, PADDING), BatchNorm2d(128),MaxPool2d(MAX_POOL_KERNEL), PReLU(), Dropout2d(droupout_conv),\n",
        "                    Conv2d(128, 256, KERNEL_SIZE_CONV, 1, PADDING), BatchNorm2d(256), MaxPool2d(MAX_POOL_KERNEL), PReLU(),Dropout2d(droupout_conv),\n",
        "                    Conv2d(256, 512, KERNEL_SIZE_CONV, 1, PADDING), BatchNorm2d(512), MaxPool2d(MAX_POOL_KERNEL), PReLU(),Dropout2d(droupout_conv),\n",
        "                    Flatten(),\n",
        "                    Linear(2048, 1024), BatchNorm1d(1024), PReLU(), Dropout(dropout_p),\n",
        "                    Linear(1024, 512), BatchNorm1d(512), PReLU(), Dropout(dropout_p),\n",
        "                    Linear(512, 256), BatchNorm1d(256), PReLU(), Dropout(dropout_p),\n",
        "                    Linear(256, 128), BatchNorm1d(128), PReLU(), Dropout(dropout_p),\n",
        "                    Linear(128, 10))\n",
        "\n",
        "  model_inference = Sequential(model, Softmax())\n",
        "  return model, model_inference\n",
        "\n",
        "\n",
        "model, model_inference = build_model()\n",
        "model, model_inference = model.to(device), model_inference.to(device)\n",
        "\n",
        "aug_transforms = transforms.Compose([transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10), transforms.ToTensor()])\n",
        "\n",
        "dataset_aug = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=aug_transforms)\n",
        "trainset_aug, _ = torch.utils.data.random_split(dataset_aug, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(trainset_aug, batch_size=batch_size, shuffle=True)\n",
        "dataloader_val = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "ce_loss = CrossEntropyLoss().to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_val_losses = []\n",
        "epoch_val_accs = []\n",
        "\n",
        "def one_epoch(model, loss, optimizer, dataloader_train, dataloader_val, device, verbose=True):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for i, batch in enumerate(dataloader_train):  \n",
        "    x, y = batch[0].to(device), batch[1].to(device) \n",
        "    optimizer.zero_grad()\n",
        "    out = model(x)\n",
        "\n",
        "    loss = ce_loss(out, y)\n",
        "    loss.backward()\n",
        "    train_losses.append(loss.item())\n",
        "    optimizer.step()\n",
        "    if i % 100 == 0 and verbose:\n",
        "      print(\"Training loss at step {}: {}\".format(i, loss.item()))\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, batch in enumerate(dataloader_val):  \n",
        "      x, y = batch[0].to(device), batch[1].to(device)  \n",
        "\n",
        "      out = model(x)\n",
        "      loss = ce_loss(out, y)\n",
        "      acc = torch.sum(torch.argmax(out, dim=-1) == y)\n",
        "      correct += acc.item()\n",
        "      total += len(batch[1])\n",
        "      val_losses.append(loss.item())\n",
        "\n",
        "  val_acc = correct / total\n",
        "\n",
        "  return np.mean(train_losses), np.mean(val_losses), val_acc\n",
        "\n",
        "for e in range(50):\n",
        "  adjust_lr(optimizer, e)\n",
        "  train_loss, val_loss, val_acc = one_epoch(model, ce_loss, optimizer, dataloader_train, dataloader_val, device, False)\n",
        "\n",
        "  print(\"Val loss at epoch {}: {}\".format(e, val_loss))\n",
        "  print(\"Val acc at epoch {}: {}\".format(e, val_acc))\n",
        "\n",
        "  epoch_train_losses.append(train_loss)\n",
        "  epoch_val_losses.append(val_loss)\n",
        "  epoch_val_accs.append(val_acc)\n",
        "\n",
        "plt.plot(epoch_train_losses, c='r')\n",
        "plt.plot(epoch_val_losses, c='b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_val_accs, c='r')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1kD1TtZj1jR"
      },
      "source": [
        "torch.save(model.state_dict(), \"{:03d}.pth\".format(50))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}